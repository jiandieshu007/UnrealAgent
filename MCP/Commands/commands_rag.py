# kvick-games/unrealmcp/UnrealMCP-f-989d0e77a4bae68c0dfc92bbcaf24ceefd06efb/MCP/Commands/commands_rag.py

"""
RAG (Retrieval-Augmented Generation) commands for the UnrealMCP bridge.
This module provides tools to query knowledge bases created from project source code and blueprints.
"""

import sys
import os
import logging
from mcp.server.fastmcp import Context

# --- RAG Core Imports ---
# These libraries are required for the RAG functionality.
# We'll wrap the main logic in a try...except block to handle missing dependencies.
try:
    from langchain_community.vectorstores import Chroma
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.runnables import RunnablePassthrough
    from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline
    
    RAG_DEPENDENCIES_MET = True

except ImportError as e:
    logging.error(f"RAG dependencies not found. Please install them. Error: {e}")
    RAG_DEPENDENCIES_MET = False

# --- Configuration ---
MCP_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
BLUEPRINT_DB_PATH = os.path.join(MCP_DIR, "ue_blueprint_chroma_db")
CPP_DB_PATH = os.path.join(MCP_DIR, "ue_cpp_chroma_db")
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
LLM_MODEL = "google/flan-t5-large"

# --- Caching for Models ---
llm_pipeline_cache = None
embedding_model_cache = None

def get_embedding_model():
    """Loads and caches the embedding model."""
    global embedding_model_cache
    if embedding_model_cache is None:
        logging.info(f"Loading embedding model: {EMBEDDING_MODEL}")
        embedding_model_cache = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        logging.info("Embedding model loaded.")
    return embedding_model_cache

def get_llm_pipeline():
    """Loads and caches the Hugging Face LLM pipeline."""
    global llm_pipeline_cache
    if llm_pipeline_cache is None:
        logging.info(f"Loading LLM pipeline: {LLM_MODEL}. This may take some time...")
        tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)
        model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL)
        pipe = pipeline(
            "text2text-generation",
            model=model,
            tokenizer=tokenizer,
            max_length=1024,
            temperature=0.7,
        )
        llm_pipeline_cache = HuggingFacePipeline(pipeline=pipe)
        logging.info("LLM pipeline loaded.")
    return llm_pipeline_cache

def register_all(mcp):
    """Register all RAG-related commands with the MCP server."""

    if not RAG_DEPENDENCIES_MET:
        # If dependencies are not met, register a dummy tool that returns an error.
        @mcp.tool()
        def query_knowledge_base(ctx: Context, query: str, source: str = "all", top_k: int = 5) -> str:
            """
            (Unavailable) Query the project's knowledge base.
            This tool is not available because required Python packages are missing.
            Please check the server logs and install the necessary dependencies.
            """
            error_message = (
                "RAG tool is not available due to missing dependencies. "
                "Please run the following command in your activated virtual environment "
                " (MCP/python_env/Scripts/activate.bat): \n"
                "pip install langchain langchain_community langchain_huggingface chromadb "
                "sentence-transformers torch transformers accelerate"
            )
            logging.error(error_message)
            return error_message
        return

    # --- If dependencies ARE met, register the fully functional tool. ---
    @mcp.tool()
    def query_knowledge_base(ctx: Context, query: str, source: str = "all", top_k: int = 5) -> str:
        """
        Query Unreal's knowledge base (C++ or Blueprints) to answer a question.
        This tool retrieves relevant information from the code/blueprint database and uses an LLM to generate an answer.

        Args:
            query: The natural language question you want to ask about the project.
            source: The knowledge base to query. Options: 'cpp', 'blueprint', or 'all'. Defaults to 'all'.
            top_k: The number of relevant documents to retrieve. Defaults to 5.
        
        Returns:
            A detailed answer generated by the LLM based on the retrieved context.
        """
        try:
            logging.info(f"Received RAG query: '{query}' for source: '{source}'")
            db_paths = []
            if source.lower() == 'cpp':
                db_paths.append(CPP_DB_PATH)
            elif source.lower() == 'blueprint':
                db_paths.append(BLUEPRINT_DB_PATH)
            elif source.lower() == 'all':
                db_paths.extend([CPP_DB_PATH, BLUEPRINT_DB_PATH])
            else:
                return f"Error: Invalid source '{source}'. Must be 'cpp', 'blueprint', or 'all'."

            embeddings = get_embedding_model()
            all_docs = []

            for db_path in db_paths:
                if not os.path.exists(db_path):
                    logging.warning(f"Database not found at {db_path}. Skipping.")
                    continue
                logging.info(f"Loading database from: {db_path}")
                db = Chroma(persist_directory=db_path, embedding_function=embeddings)
                retriever = db.as_retriever(search_kwargs={"k": top_k})
                docs = retriever.get_relevant_documents(query)
                all_docs.extend(docs)
                logging.info(f"Retrieved {len(docs)} documents from {os.path.basename(db_path)}.")
            
            if not all_docs:
                return "I couldn't find any relevant information in the knowledge base to answer your question."

            context_text = "\n\n---\n\n".join([doc.page_content for doc in all_docs])
            template = """
            You are an expert Unreal Engine assistant. Use the following context from the project's C++ code and Blueprints to answer the question.
            Provide detailed, code-centric answers. If the context includes code snippets, use them in your answer.
            If the answer isn't in the context, say that you couldn't find relevant information in the project's knowledge base.

            Context:
            {context}

            Question:
            {question}

            Answer:
            """
            prompt = ChatPromptTemplate.from_template(template)
            llm = get_llm_pipeline()
            rag_chain = (prompt | llm | StrOutputParser())

            logging.info("Invoking RAG chain to generate answer...")
            answer = rag_chain.invoke({"context": context_text, "question": query})
            logging.info("RAG answer generated successfully.")
            
            return answer

        except Exception as e:
            logging.error(f"Error during RAG query: {str(e)}", exc_info=True)
            return f"An error occurred while querying the knowledge base: {str(e)}"